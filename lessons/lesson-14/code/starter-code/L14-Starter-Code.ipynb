{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals # unused in this notebook\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# spacy is used for pre-processing and traditional NLP\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "\n",
    "# Word2Vec - find words that are most similar to certain words\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# LDA - \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus # need to use for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "data = pd.read_csv(\"../dataset/stumbleupon.tsv\", sep='\\t')\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "data.head()\n",
    "\n",
    "\n",
    "# do countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer   \n",
    "cv = CountVectorizer(binary=False,   stop_words='english',  min_df=3)\n",
    "\n",
    "docs = cv.fit_transform(data.body.dropna())\n",
    "# Build a mapping of numerical ID to word\n",
    "id2word = dict(enumerate(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we convert our word-matrix into gensim's format \n",
    "corpus = Sparse2Corpus(docs, documents_columns = False)\n",
    "\n",
    "# Then we fit an LDA model \n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics= 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  u'0.009*\"image\" + 0.008*\"small\" + 0.008*\"images\" + 0.008*\"2011\" + 0.008*\"link\" + 0.007*\"buzz\" + 0.007*\"chocolate\" + 0.006*\"campaign\" + 0.006*\"track\" + 0.006*\"jpg\"'),\n",
       " (0,\n",
       "  u'0.008*\"2010\" + 0.006*\"2009\" + 0.006*\"technology\" + 0.005*\"new\" + 0.005*\"2008\" + 0.004*\"future\" + 0.004*\"2007\" + 0.004*\"2011\" + 0.004*\"like\" + 0.004*\"2006\"'),\n",
       " (10,\n",
       "  u'0.007*\"food\" + 0.007*\"make\" + 0.007*\"recipe\" + 0.006*\"water\" + 0.006*\"salad\" + 0.006*\"like\" + 0.005*\"cheese\" + 0.005*\"eggs\" + 0.005*\"recipes\" + 0.005*\"bacon\"'),\n",
       " (9,\n",
       "  u'0.008*\"apple\" + 0.007*\"like\" + 0.006*\"recipe\" + 0.005*\"just\" + 0.004*\"time\" + 0.004*\"make\" + 0.004*\"food\" + 0.003*\"google\" + 0.003*\"data\" + 0.003*\"recipes\"'),\n",
       " (12,\n",
       "  u'0.013*\"dress\" + 0.010*\"clothing\" + 0.009*\"indie\" + 0.006*\"nav\" + 0.006*\"background\" + 0.005*\"00\" + 0.005*\"url\" + 0.005*\"dresses\" + 0.005*\"images\" + 0.004*\"google\"'),\n",
       " (4,\n",
       "  u'0.008*\"health\" + 0.007*\"body\" + 0.006*\"people\" + 0.004*\"help\" + 0.004*\"weight\" + 0.004*\"time\" + 0.004*\"brain\" + 0.003*\"like\" + 0.003*\"fat\" + 0.003*\"just\"'),\n",
       " (6,\n",
       "  u'0.007*\"just\" + 0.007*\"fashion\" + 0.005*\"year\" + 0.005*\"like\" + 0.004*\"new\" + 0.004*\"sports\" + 0.004*\"time\" + 0.004*\"2011\" + 0.004*\"love\" + 0.003*\"make\"'),\n",
       " (2,\n",
       "  u'0.013*\"cup\" + 0.009*\"minutes\" + 0.009*\"recipe\" + 0.008*\"add\" + 0.008*\"butter\" + 0.007*\"make\" + 0.006*\"baking\" + 0.006*\"just\" + 0.006*\"oven\" + 0.006*\"salt\"'),\n",
       " (13,\n",
       "  u'0.006*\"funny\" + 0.005*\"new\" + 0.004*\"just\" + 0.004*\"news\" + 0.004*\"videos\" + 0.003*\"like\" + 0.003*\"art\" + 0.003*\"said\" + 0.003*\"left\" + 0.003*\"video\"'),\n",
       " (8,\n",
       "  u'0.028*\"com\" + 0.021*\"online\" + 0.020*\"www\" + 0.018*\"http\" + 0.015*\"guide\" + 0.011*\"damascus\" + 0.011*\"swimsuit\" + 0.010*\"si\" + 0.007*\"sports\" + 0.007*\"models\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "(0, u'0.008*\"2010\" + 0.006*\"2009\" + 0.006*\"technology\" + 0.005*\"new\" + 0.005*\"2008\"')\n",
      "()\n",
      "Topic: 1\n",
      "(1, u'0.016*\"http\" + 0.016*\"com\" + 0.012*\"href\" + 0.011*\"www\" + 0.006*\"la\"')\n",
      "()\n",
      "Topic: 2\n",
      "(2, u'0.013*\"cup\" + 0.009*\"minutes\" + 0.009*\"recipe\" + 0.008*\"add\" + 0.008*\"butter\"')\n",
      "()\n",
      "Topic: 3\n",
      "(3, u'0.009*\"image\" + 0.008*\"small\" + 0.008*\"images\" + 0.008*\"2011\" + 0.008*\"link\"')\n",
      "()\n",
      "Topic: 4\n",
      "(4, u'0.008*\"health\" + 0.007*\"body\" + 0.006*\"people\" + 0.004*\"help\" + 0.004*\"weight\"')\n",
      "()\n",
      "Topic: 5\n",
      "(5, u'0.019*\"chocolate\" + 0.018*\"cake\" + 0.011*\"sugar\" + 0.009*\"butter\" + 0.009*\"cream\"')\n",
      "()\n",
      "Topic: 6\n",
      "(6, u'0.007*\"just\" + 0.007*\"fashion\" + 0.005*\"year\" + 0.005*\"like\" + 0.004*\"new\"')\n",
      "()\n",
      "Topic: 7\n",
      "(7, u'0.005*\"cancer\" + 0.005*\"world\" + 0.004*\"content\" + 0.004*\"like\" + 0.004*\"olympics\"')\n",
      "()\n",
      "Topic: 8\n",
      "(8, u'0.028*\"com\" + 0.021*\"online\" + 0.020*\"www\" + 0.018*\"http\" + 0.015*\"guide\"')\n",
      "()\n",
      "Topic: 9\n",
      "(9, u'0.008*\"apple\" + 0.007*\"like\" + 0.006*\"recipe\" + 0.005*\"just\" + 0.004*\"time\"')\n",
      "()\n",
      "Topic: 10\n",
      "(10, u'0.007*\"food\" + 0.007*\"make\" + 0.007*\"recipe\" + 0.006*\"water\" + 0.006*\"salad\"')\n",
      "()\n",
      "Topic: 11\n",
      "(11, u'0.018*\"raw\" + 0.017*\"food\" + 0.010*\"foods\" + 0.009*\"healthy\" + 0.007*\"chocolate\"')\n",
      "()\n",
      "Topic: 12\n",
      "(12, u'0.013*\"dress\" + 0.010*\"clothing\" + 0.009*\"indie\" + 0.006*\"nav\" + 0.006*\"background\"')\n",
      "()\n",
      "Topic: 13\n",
      "(13, u'0.006*\"funny\" + 0.005*\"new\" + 0.004*\"just\" + 0.004*\"news\" + 0.004*\"videos\"')\n",
      "()\n",
      "Topic: 14\n",
      "(14, u'0.008*\"said\" + 0.007*\"flashvars\" + 0.005*\"like\" + 0.004*\"just\" + 0.004*\"time\"')\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# characterizing what i want to see\n",
    "# see 5 most important words in each topic\n",
    "\n",
    "num_topics = 25\n",
    "n_words_per_topic = 5 \n",
    "for ti, topic in enumerate(lda_model.show_topics(num_topics = num_topics, num_words = n_words_per_topic)):\n",
    "    print(\"Topic: %d\" % (ti))\n",
    "    print (topic)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Setup the body text \n",
    "text = data.body.dropna().map(lambda x: x.split()) \n",
    "\n",
    "from gensim.models import Word2Vec \n",
    "model = Word2Vec(text, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'candy', 0.8733401894569397),\n",
       " (u'frosting', 0.8526742458343506),\n",
       " (u'cake', 0.8476889729499817),\n",
       " (u'buttercream', 0.8459649085998535),\n",
       " (u'ganache', 0.8397178053855896),\n",
       " (u'caramel', 0.8356987833976746),\n",
       " (u'chips', 0.8320656418800354),\n",
       " (u'pudding', 0.8314386606216431),\n",
       " (u'marshmallows', 0.8285681009292603),\n",
       " (u'fudge', 0.8231680393218994)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['chocolate', 'brownie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the tweet data\n",
    "filename = '../dataset/captured-tweets.txt'\n",
    "tweets = []\n",
    "for tweet in codecs.open(filename, 'r', encoding=\"utf-8\"):\n",
    "    tweets.append(tweet)\n",
    "    \n",
    "# Setting up spacy\n",
    "nlp_toolkit = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1a\n",
    "\n",
    "Write a function that can take a take a sentence parsed by `spacy` and identify if it mentions a company named 'Google'. Remember, `spacy` can find entities and codes them as `ORG` if they are a company. Look at the slides for class 13 if you need a hint:\n",
    "\n",
    "### Bonus (1b)\n",
    "\n",
    "Parameterize the company name so that the function works for any company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentions_company(parsed):\n",
    "    parsed = nlp_toolkit(parsed)\n",
    "    # Return True if the sentence contains an organization and that organization is Google\n",
    "    #print parsed\n",
    "    for entity in parsed.ents:\n",
    "        if 'google' in entity.text.lower():\n",
    "            return True\n",
    "    # Otherwise return False\n",
    "    return False\n",
    "\n",
    "\n",
    "# 1b\n",
    "def mentions_company(parsed, search_term='Google'):\n",
    "    parsed = nlp_toolkit(parsed)\n",
    "    # Return True if the sentence contains an organization and that organization is Google\n",
    "    #print parsed\n",
    "    for entity in parsed.ents:\n",
    "        if search_term.lower() in entity.text.lower():\n",
    "            return True\n",
    "    # Otherwise return False\n",
    "    return False\n",
    "\n",
    "\n",
    "relevant_tweets=[]\n",
    "for tweet in tweets:\n",
    "    if mentions_company(tweet):\n",
    "        relevant_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1c\n",
    "\n",
    "Write a function that can take a sentence parsed by `spacy` \n",
    "and return the verbs of the sentence (preferably lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1d\n",
    "For each tweet, parse it using spacy and print it out if the tweet has 'release' or 'announce' as a verb. You'll need to use your `mentions_company` and `get_actions` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google &amp; Ford rumored to announce partnership at CES https://t.co/zOgm1NjHhD https://t.co/Gzx81ujqVC\n",
      "\n",
      "Google's Project Ara Spiral is expected to be released next year January https://t.co/prycPMuGsG\n",
      "\n",
      "Google and Ford to announce partnership on self-driving cars at CES - Fudzilla (blog) https://t.co/6woe56G22Q\n",
      "\n",
      "Google and Ford to announce partnership on self-driving cars at CES - Fudzilla (blog) https://t.co/4hERVJ4zZK\n",
      "\n",
      "Redesigned Google Glass published on FCC website: release date, Price and features - Tampa Bay Review https://t.co/Vdwr4afx3E www.GlassRooâ€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_actions(parsed):\n",
    "    actions = []\n",
    "    # Your code here\n",
    "    parsed = nlp_toolkit(parsed)\n",
    "    for (i, word) in enumerate(parsed): \n",
    "        if word.lemma_ =='announce' or word.lemma_ ==\"release\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "for tweet in relevant_tweets:\n",
    "    if get_actions(tweet):\n",
    "        print tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1e\n",
    "Write a function that identifies countries - HINT: the entity label for countries is GPE (or GeoPolitical Entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_country(parsed, country):\n",
    "    parsed = nlp_toolkit(parsed)\n",
    "    for word in parsed:\n",
    "        if word.text == 'country' and word.ent_type_ == 'GPE':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for tweet in tweets:\n",
    "    if mentions_country(tweet, 'Japan'):\n",
    "        print tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1f\n",
    "\n",
    "Re-run (d) to find country tweets that discuss 'Iran' announcing or releasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Build a `word2vec` model of the tweets we have collected using `gensim`.\n",
    "\n",
    "### Exercise 2a:\n",
    "First take the collection of tweets and tokenize them using spacy.\n",
    "\n",
    "* Think about how this should be done. \n",
    "* Should you only use upper-case or lower-case? \n",
    "* Should you remove punctuations or symbols? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = [[x.text if x.pos != spacy.parts_of_speech.VERB else x.lemma_ \n",
    "                for x in nlp_toolkit(t)] for t in tweets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b:\n",
    "Build a `word2vec` model.\n",
    "Test the window size as well - this is how many surrounding words need to be used to model a word. What do you think is appropriate for Twitter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(text_split, size=100, window=4, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2c:\n",
    "Test your word2vec model with a few similarity functions. \n",
    "* Find words similar to 'Syria'.\n",
    "* Find words similar to 'war'.\n",
    "* Find words similar to \"Iran\".\n",
    "* Find words similar to 'Verizon'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Syria'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2d\n",
    "\n",
    "Adjust the choices / parameters in (b) and (c) as necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Filter tweets to those that mention 'Iran' or similar entities and 'war' or similar entities.\n",
    "* Do this using just spacy.\n",
    "* Do this using word2vec similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using spacy\n",
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using word2vec similarity scores\n",
    "for tweet in tweets[:200]:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
